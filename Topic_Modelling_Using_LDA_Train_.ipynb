{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Topic Modelling Using LDA Train .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revs1/Janatahack---AV-Hackathon-Topic-Modelling/blob/master/Topic_Modelling_Using_LDA_Train_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgduZIaBaS3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn import decomposition \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP7V_2eCbCX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dd7452d3-99e3-4fb2-cca9-6690bc8277f7"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5Q1GZ5B-az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/sample_data/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYyjGb_deCEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "6478998e-ac3f-4c5b-c205-e90eb9938dad"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20967</th>\n",
              "      <td>20968</td>\n",
              "      <td>Contemporary machine learning: a guide for pra...</td>\n",
              "      <td>Machine learning is finding increasingly bro...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20968</th>\n",
              "      <td>20969</td>\n",
              "      <td>Uniform diamond coatings on WC-Co hard alloy c...</td>\n",
              "      <td>Polycrystalline diamond coatings have been g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20969</th>\n",
              "      <td>20970</td>\n",
              "      <td>Analysing Soccer Games with Clustering and Con...</td>\n",
              "      <td>We present a new approach for identifying si...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20970</th>\n",
              "      <td>20971</td>\n",
              "      <td>On the Efficient Simulation of the Left-Tail o...</td>\n",
              "      <td>The sum of Log-normal variates is encountere...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20971</th>\n",
              "      <td>20972</td>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subje...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20972 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ... Quantitative Finance\n",
              "0          1  ...                    0\n",
              "1          2  ...                    0\n",
              "2          3  ...                    0\n",
              "3          4  ...                    0\n",
              "4          5  ...                    0\n",
              "...      ...  ...                  ...\n",
              "20967  20968  ...                    0\n",
              "20968  20969  ...                    0\n",
              "20969  20970  ...                    0\n",
              "20970  20971  ...                    0\n",
              "20971  20972  ...                    0\n",
              "\n",
              "[20972 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj1OrjpHDJEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting binary column to category\n",
        "to_convert = ['Computer Science', 'Physics', 'Mathematics','Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
        "\n",
        "# Make a copy of train data\n",
        "topic_data = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tViYQQ0cDIe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changing the binary fields to categorical fields\n",
        "topic_data = topic_data[topic_data[to_convert]==1].stack().reset_index().drop(0,1)\n",
        "\n",
        "topic_data['ID'] = topic_data['level_0'].apply(lambda x: x+1)\n",
        "topic_data = topic_data.drop('level_0', axis=1)\n",
        "\n",
        "# Merge the data based on ID\n",
        "merge_data = df.merge(topic_data, how='left', on='ID' )\n",
        "# Drop all the binary fields\n",
        "merge_data = merge_data.drop(to_convert, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5HrdP-kDIZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e8675392-bfea-4d3f-b664-fab80cea7999"
      },
      "source": [
        "# Rename the column to Category\n",
        "merge_data = merge_data.rename({'level_1':'CATEGORY'}, axis=1)\n",
        "merge_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26262</th>\n",
              "      <td>20970</td>\n",
              "      <td>Analysing Soccer Games with Clustering and Con...</td>\n",
              "      <td>We present a new approach for identifying si...</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26263</th>\n",
              "      <td>20971</td>\n",
              "      <td>On the Efficient Simulation of the Left-Tail o...</td>\n",
              "      <td>The sum of Log-normal variates is encountere...</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26264</th>\n",
              "      <td>20971</td>\n",
              "      <td>On the Efficient Simulation of the Left-Tail o...</td>\n",
              "      <td>The sum of Log-normal variates is encountere...</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26265</th>\n",
              "      <td>20972</td>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subje...</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26266</th>\n",
              "      <td>20972</td>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subje...</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26267 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...          CATEGORY\n",
              "0          1  ...  Computer Science\n",
              "1          2  ...  Computer Science\n",
              "2          3  ...       Mathematics\n",
              "3          4  ...       Mathematics\n",
              "4          5  ...  Computer Science\n",
              "...      ...  ...               ...\n",
              "26262  20970  ...  Computer Science\n",
              "26263  20971  ...       Mathematics\n",
              "26264  20971  ...        Statistics\n",
              "26265  20972  ...       Mathematics\n",
              "26266  20972  ...        Statistics\n",
              "\n",
              "[26267 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exVouIKOhnzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2f79b06f-7b36-4956-f867-e36fe5059b9b"
      },
      "source": [
        "merge_data['CATEGORY'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Computer Science        8594\n",
              "Physics                 6013\n",
              "Mathematics             5618\n",
              "Statistics              5206\n",
              "Quantitative Biology     587\n",
              "Quantitative Finance     249\n",
              "Name: CATEGORY, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfaX38U7j-O5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89d86e27-ab58-414e-bd4f-69d065b9a557"
      },
      "source": [
        "df = merge_data.copy()\n",
        "df.columns\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'TITLE', 'ABSTRACT', 'CATEGORY'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct8Zb921kmYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a875f49-5632-43ac-ed3a-d6164c86a4fc"
      },
      "source": [
        "pd.set_option('display.max_colwidth',-1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels for polyharmonic functions</td>\n",
              "      <td>We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26262</th>\n",
              "      <td>20970</td>\n",
              "      <td>Analysing Soccer Games with Clustering and Conceptors</td>\n",
              "      <td>We present a new approach for identifying situations and behaviours, which we\\ncall \"moves\", from soccer games in the 2D simulation league. Being able to\\nidentify key situations and behaviours are useful capabilities for analysing\\nsoccer matches, anticipating opponent behaviours to aid selection of\\nappropriate tactics, and also as a prerequisite for automatic learning of\\nbehaviours and policies. To support a wide set of strategies, our goal is to\\nidentify situations from data, in an unsupervised way without making use of\\npre-defined soccer specific concepts such as \"pass\" or \"dribble\". The recurrent\\nneural networks we use in our approach act as a high-dimensional projection of\\nthe recent history of a situation on the field. Similar situations, i.e., with\\nsimilar histories, are found by clustering of network states. The same networks\\nare also used to learn so-called conceptors, that are lower-dimensional\\nmanifolds that describe trajectories through a high-dimensional state space\\nthat enable situation-specific predictions from the same neural network. With\\nthe proposed approach, we can segment games into sequences of situations that\\nare learnt in an unsupervised way, and learn conceptors that are useful for the\\nprediction of the near future of the respective situation.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26263</th>\n",
              "      <td>20971</td>\n",
              "      <td>On the Efficient Simulation of the Left-Tail of the Sum of Correlated Log-normal Variates</td>\n",
              "      <td>The sum of Log-normal variates is encountered in many challenging\\napplications such as in performance analysis of wireless communication systems\\nand in financial engineering. Several approximation methods have been developed\\nin the literature, the accuracy of which is not ensured in the tail regions.\\nThese regions are of primordial interest wherein small probability values have\\nto be evaluated with high precision. Variance reduction techniques are known to\\nyield accurate, yet efficient, estimates of small probability values. Most of\\nthe existing approaches, however, have considered the problem of estimating the\\nright-tail of the sum of Log-normal random variables (RVS). In the present\\nwork, we consider instead the estimation of the left-tail of the sum of\\ncorrelated Log-normal variates with Gaussian copula under a mild assumption on\\nthe covariance matrix. We propose an estimator combining an existing\\nmean-shifting importance sampling approach with a control variate technique.\\nThe main result is that the proposed estimator has an asymptotically vanishing\\nrelative error which represents a major finding in the context of the left-tail\\nsimulation of the sum of Log-normal RVs. Finally, we assess by various\\nsimulation results the performances of the proposed estimator compared to\\nexisting estimators.\\n</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26264</th>\n",
              "      <td>20971</td>\n",
              "      <td>On the Efficient Simulation of the Left-Tail of the Sum of Correlated Log-normal Variates</td>\n",
              "      <td>The sum of Log-normal variates is encountered in many challenging\\napplications such as in performance analysis of wireless communication systems\\nand in financial engineering. Several approximation methods have been developed\\nin the literature, the accuracy of which is not ensured in the tail regions.\\nThese regions are of primordial interest wherein small probability values have\\nto be evaluated with high precision. Variance reduction techniques are known to\\nyield accurate, yet efficient, estimates of small probability values. Most of\\nthe existing approaches, however, have considered the problem of estimating the\\nright-tail of the sum of Log-normal random variables (RVS). In the present\\nwork, we consider instead the estimation of the left-tail of the sum of\\ncorrelated Log-normal variates with Gaussian copula under a mild assumption on\\nthe covariance matrix. We propose an estimator combining an existing\\nmean-shifting importance sampling approach with a control variate technique.\\nThe main result is that the proposed estimator has an asymptotically vanishing\\nrelative error which represents a major finding in the context of the left-tail\\nsimulation of the sum of Log-normal RVs. Finally, we assess by various\\nsimulation results the performances of the proposed estimator compared to\\nexisting estimators.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26265</th>\n",
              "      <td>20972</td>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subject of debate in the Bayesian\\npsychology community. Rouder (2014) argues that optional stopping is no problem\\nfor Bayesians, and even recommends the use of optional stopping in practice, as\\ndo Wagenmakers et al. (2012). This article addresses the question whether\\noptional stopping is problematic for Bayesian methods, and specifies under\\nwhich circumstances and in which sense it is and is not. By slightly varying\\nand extending Rouder's (2014) experiment, we illustrate that, as soon as the\\nparameters of interest are equipped with default or pragmatic priors - which\\nmeans, in most practical applications of Bayes Factor hypothesis testing -\\nresilience to optional stopping can break down. We distinguish between four\\ntypes of default priors, each having their own specific issues with optional\\nstopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type\\nII and III priors).\\n</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26266</th>\n",
              "      <td>20972</td>\n",
              "      <td>Why optional stopping is a problem for Bayesians</td>\n",
              "      <td>Recently, optional stopping has been a subject of debate in the Bayesian\\npsychology community. Rouder (2014) argues that optional stopping is no problem\\nfor Bayesians, and even recommends the use of optional stopping in practice, as\\ndo Wagenmakers et al. (2012). This article addresses the question whether\\noptional stopping is problematic for Bayesian methods, and specifies under\\nwhich circumstances and in which sense it is and is not. By slightly varying\\nand extending Rouder's (2014) experiment, we illustrate that, as soon as the\\nparameters of interest are equipped with default or pragmatic priors - which\\nmeans, in most practical applications of Bayes Factor hypothesis testing -\\nresilience to optional stopping can break down. We distinguish between four\\ntypes of default priors, each having their own specific issues with optional\\nstopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type\\nII and III priors).\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26267 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...          CATEGORY\n",
              "0      1      ...  Computer Science\n",
              "1      2      ...  Computer Science\n",
              "2      3      ...  Mathematics     \n",
              "3      4      ...  Mathematics     \n",
              "4      5      ...  Computer Science\n",
              "...   ..      ...               ...\n",
              "26262  20970  ...  Computer Science\n",
              "26263  20971  ...  Mathematics     \n",
              "26264  20971  ...  Statistics      \n",
              "26265  20972  ...  Mathematics     \n",
              "26266  20972  ...  Statistics      \n",
              "\n",
              "[26267 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKX5_dgUk1aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_hold = train_test_split(df,test_size=0.6,random_state = 111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tACZHokblAnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7acbdbbf-a87d-4c1f-e4f5-4745db76c778"
      },
      "source": [
        "X_train['CATEGORY'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Computer Science        3456\n",
              "Physics                 2353\n",
              "Mathematics             2242\n",
              "Statistics              2139\n",
              "Quantitative Biology    224 \n",
              "Quantitative Finance    92  \n",
              "Name: CATEGORY, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZpcdhX4lHPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBhlD-jblXWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  tokens = [word for word in nltk.word_tokenize(text) if (len(word)>3 and len(word.strip('Xx/'))>2)]\n",
        "  #stems = [stemmer.stem(item) for item in tokens]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqMOqPaymnPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer_tf = TfidfVectorizer(tokenizer= tokenize,stop_words='english',max_df=0.75,min_df=50,max_features=10000,use_idf=False,norm=None)\n",
        "tf_vectors = vectorizer_tf.fit_transform(X_train.ABSTRACT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBNUdtGWnXgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4a23e614-4fb4-483f-c071-87e93523e220"
      },
      "source": [
        "tf_vectors.A"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E0PRVkCoRQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11c3d67c-0b5c-4acd-e3a9-89acf1241232"
      },
      "source": [
        "vectorizer_tf.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-dimensional',\n",
              " '2013',\n",
              " '2014',\n",
              " '2015',\n",
              " '2016',\n",
              " '2017',\n",
              " '\\\\alpha',\n",
              " '\\\\beta',\n",
              " '\\\\cite',\n",
              " '\\\\delta',\n",
              " '\\\\emph',\n",
              " '\\\\epsilon',\n",
              " '\\\\frac',\n",
              " '\\\\gamma',\n",
              " '\\\\geq',\n",
              " '\\\\infty',\n",
              " '\\\\lambda',\n",
              " '\\\\ldots',\n",
              " '\\\\leq',\n",
              " '\\\\log',\n",
              " '\\\\mathbb',\n",
              " '\\\\mathcal',\n",
              " '\\\\mathrm',\n",
              " '\\\\omega',\n",
              " '\\\\right',\n",
              " '\\\\sigma',\n",
              " '\\\\sim',\n",
              " '\\\\sqrt',\n",
              " '\\\\textit',\n",
              " '\\\\theta',\n",
              " '\\\\times',\n",
              " 'ability',\n",
              " 'able',\n",
              " 'absence',\n",
              " 'absolute',\n",
              " 'absorption',\n",
              " 'abstract',\n",
              " 'accelerated',\n",
              " 'acceleration',\n",
              " 'access',\n",
              " 'according',\n",
              " 'account',\n",
              " 'accounts',\n",
              " 'accuracy',\n",
              " 'accurate',\n",
              " 'accurately',\n",
              " 'achievable',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achieves',\n",
              " 'achieving',\n",
              " 'acoustic',\n",
              " 'acquired',\n",
              " 'acquisition',\n",
              " 'action',\n",
              " 'actions',\n",
              " 'activation',\n",
              " 'active',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'adapt',\n",
              " 'adaptation',\n",
              " 'adapted',\n",
              " 'adaptive',\n",
              " 'added',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'additionally',\n",
              " 'additive',\n",
              " 'address',\n",
              " 'addressed',\n",
              " 'addresses',\n",
              " 'addressing',\n",
              " 'admit',\n",
              " 'admits',\n",
              " 'adopt',\n",
              " 'adopted',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advances',\n",
              " 'advantage',\n",
              " 'advantages',\n",
              " 'adversarial',\n",
              " 'affect',\n",
              " 'affected',\n",
              " 'affects',\n",
              " 'affine',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'agreement',\n",
              " 'aimed',\n",
              " 'aims',\n",
              " 'algebra',\n",
              " 'algebraic',\n",
              " 'algebras',\n",
              " 'algorithm',\n",
              " 'algorithmic',\n",
              " 'algorithms',\n",
              " 'alignment',\n",
              " 'allocation',\n",
              " 'allow',\n",
              " 'allowed',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'alternating',\n",
              " 'alternative',\n",
              " 'alternatives',\n",
              " 'amounts',\n",
              " 'amplitude',\n",
              " 'analogous',\n",
              " 'analyse',\n",
              " 'analyses',\n",
              " 'analysis',\n",
              " 'analytic',\n",
              " 'analytical',\n",
              " 'analytically',\n",
              " 'analyze',\n",
              " 'analyzed',\n",
              " 'analyzing',\n",
              " 'and/or',\n",
              " 'angle',\n",
              " 'angular',\n",
              " 'anisotropic',\n",
              " 'anisotropy',\n",
              " 'anomalous',\n",
              " 'anomaly',\n",
              " 'answer',\n",
              " 'appear',\n",
              " 'appears',\n",
              " 'applicability',\n",
              " 'applicable',\n",
              " 'application',\n",
              " 'applications',\n",
              " 'applied',\n",
              " 'applies',\n",
              " 'apply',\n",
              " 'applying',\n",
              " 'approach',\n",
              " 'approaches',\n",
              " 'appropriate',\n",
              " 'approximate',\n",
              " 'approximated',\n",
              " 'approximately',\n",
              " 'approximating',\n",
              " 'approximation',\n",
              " 'approximations',\n",
              " 'arbitrarily',\n",
              " 'arbitrary',\n",
              " 'architecture',\n",
              " 'architectures',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'argue',\n",
              " 'argument',\n",
              " 'arguments',\n",
              " 'arise',\n",
              " 'arises',\n",
              " 'arising',\n",
              " 'arithmetic',\n",
              " 'array',\n",
              " 'article',\n",
              " 'artificial',\n",
              " 'aspect',\n",
              " 'aspects',\n",
              " 'assess',\n",
              " 'assessed',\n",
              " 'assessment',\n",
              " 'associated',\n",
              " 'association',\n",
              " 'assume',\n",
              " 'assumed',\n",
              " 'assuming',\n",
              " 'assumption',\n",
              " 'assumptions',\n",
              " 'asymmetric',\n",
              " 'asymptotic',\n",
              " 'asymptotically',\n",
              " 'atomic',\n",
              " 'atoms',\n",
              " 'attack',\n",
              " 'attacks',\n",
              " 'attempt',\n",
              " 'attempts',\n",
              " 'attention',\n",
              " 'attracted',\n",
              " 'attractive',\n",
              " 'attributes',\n",
              " 'augmented',\n",
              " 'author',\n",
              " 'authors',\n",
              " 'autoencoder',\n",
              " 'automated',\n",
              " 'automatic',\n",
              " 'automatically',\n",
              " 'autonomous',\n",
              " 'auxiliary',\n",
              " 'availability',\n",
              " 'available',\n",
              " 'average',\n",
              " 'averaging',\n",
              " 'avoid',\n",
              " 'avoiding',\n",
              " 'avoids',\n",
              " 'away',\n",
              " 'background',\n",
              " 'balance',\n",
              " 'ball',\n",
              " 'band',\n",
              " 'bandit',\n",
              " 'bands',\n",
              " 'bandwidth',\n",
              " 'base',\n",
              " 'based',\n",
              " 'baseline',\n",
              " 'baselines',\n",
              " 'basic',\n",
              " 'basis',\n",
              " 'batch',\n",
              " 'bayes',\n",
              " 'bayesian',\n",
              " 'beam',\n",
              " 'behavior',\n",
              " 'behaviors',\n",
              " 'behaviour',\n",
              " 'belief',\n",
              " 'believe',\n",
              " 'benchmark',\n",
              " 'benchmarks',\n",
              " 'benefit',\n",
              " 'benefits',\n",
              " 'best',\n",
              " 'better',\n",
              " 'bias',\n",
              " 'biased',\n",
              " 'binary',\n",
              " 'biological',\n",
              " 'bipartite',\n",
              " 'black',\n",
              " 'black-box',\n",
              " 'block',\n",
              " 'blocks',\n",
              " 'body',\n",
              " 'boltzmann',\n",
              " 'bottleneck',\n",
              " 'bound',\n",
              " 'boundaries',\n",
              " 'boundary',\n",
              " 'bounded',\n",
              " 'bounds',\n",
              " 'brain',\n",
              " 'break',\n",
              " 'breaking',\n",
              " 'bridge',\n",
              " 'broad',\n",
              " 'brownian',\n",
              " 'build',\n",
              " 'building',\n",
              " 'builds',\n",
              " 'built',\n",
              " 'bulk',\n",
              " 'business',\n",
              " 'calculate',\n",
              " 'calculated',\n",
              " 'calculation',\n",
              " 'calculations',\n",
              " 'calibration',\n",
              " 'called',\n",
              " 'camera',\n",
              " 'cancer',\n",
              " 'candidate',\n",
              " 'candidates',\n",
              " 'canonical',\n",
              " 'capabilities',\n",
              " 'capability',\n",
              " 'capable',\n",
              " 'capacity',\n",
              " 'capture',\n",
              " 'captured',\n",
              " 'captures',\n",
              " 'capturing',\n",
              " 'care',\n",
              " 'careful',\n",
              " 'carefully',\n",
              " 'carlo',\n",
              " 'carried',\n",
              " 'carry',\n",
              " 'case',\n",
              " 'cases',\n",
              " 'categories',\n",
              " 'category',\n",
              " 'causal',\n",
              " 'cause',\n",
              " 'caused',\n",
              " 'causes',\n",
              " 'cell',\n",
              " 'cells',\n",
              " 'cellular',\n",
              " 'center',\n",
              " 'central',\n",
              " 'certain',\n",
              " 'chain',\n",
              " 'chains',\n",
              " 'challenge',\n",
              " 'challenges',\n",
              " 'challenging',\n",
              " 'change',\n",
              " 'changes',\n",
              " 'changing',\n",
              " 'channel',\n",
              " 'channels',\n",
              " 'chaotic',\n",
              " 'character',\n",
              " 'characteristic',\n",
              " 'characteristics',\n",
              " 'characterization',\n",
              " 'characterize',\n",
              " 'characterized',\n",
              " 'characterizing',\n",
              " 'charge',\n",
              " 'chemical',\n",
              " 'choice',\n",
              " 'choices',\n",
              " 'choose',\n",
              " 'choosing',\n",
              " 'chosen',\n",
              " 'cifar-10',\n",
              " 'circular',\n",
              " 'class',\n",
              " 'classes',\n",
              " 'classic',\n",
              " 'classical',\n",
              " 'classification',\n",
              " 'classified',\n",
              " 'classifier',\n",
              " 'classifiers',\n",
              " 'classify',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'clinical',\n",
              " 'close',\n",
              " 'closed',\n",
              " 'closely',\n",
              " 'cloud',\n",
              " 'cluster',\n",
              " 'clustering',\n",
              " 'clusters',\n",
              " 'cnns',\n",
              " 'code',\n",
              " 'codes',\n",
              " 'coding',\n",
              " 'coefficient',\n",
              " 'coefficients',\n",
              " 'cognitive',\n",
              " 'coherence',\n",
              " 'coherent',\n",
              " 'cold',\n",
              " 'collaborative',\n",
              " 'collect',\n",
              " 'collected',\n",
              " 'collection',\n",
              " 'collective',\n",
              " 'collision',\n",
              " 'color',\n",
              " 'combination',\n",
              " 'combinations',\n",
              " 'combinatorial',\n",
              " 'combine',\n",
              " 'combined',\n",
              " 'combines',\n",
              " 'combining',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'coming',\n",
              " 'common',\n",
              " 'commonly',\n",
              " 'communication',\n",
              " 'communications',\n",
              " 'communities',\n",
              " 'community',\n",
              " 'compact',\n",
              " 'comparable',\n",
              " 'compare',\n",
              " 'compared',\n",
              " 'comparing',\n",
              " 'comparison',\n",
              " 'comparisons',\n",
              " 'compatible',\n",
              " 'competing',\n",
              " 'competitive',\n",
              " 'complement',\n",
              " 'complementary',\n",
              " 'complete',\n",
              " 'completely',\n",
              " 'completion',\n",
              " 'complex',\n",
              " 'complexity',\n",
              " 'complicated',\n",
              " 'component',\n",
              " 'components',\n",
              " 'composed',\n",
              " 'composition',\n",
              " 'compound',\n",
              " 'comprehensive',\n",
              " 'compressed',\n",
              " 'compression',\n",
              " 'computation',\n",
              " 'computational',\n",
              " 'computationally',\n",
              " 'computations',\n",
              " 'compute',\n",
              " 'computed',\n",
              " 'computer',\n",
              " 'computing',\n",
              " 'concentration',\n",
              " 'concept',\n",
              " 'concepts',\n",
              " 'concerned',\n",
              " 'concerning',\n",
              " 'conclude',\n",
              " 'conclusion',\n",
              " 'conclusions',\n",
              " 'condition',\n",
              " 'conditional',\n",
              " 'conditions',\n",
              " 'conduct',\n",
              " 'conducted',\n",
              " 'confidence',\n",
              " 'configuration',\n",
              " 'configurations',\n",
              " 'confirm',\n",
              " 'confirmed',\n",
              " 'conjecture',\n",
              " 'connected',\n",
              " 'connection',\n",
              " 'connections',\n",
              " 'connectivity',\n",
              " 'consensus',\n",
              " 'consequence',\n",
              " 'consequences',\n",
              " 'consequently',\n",
              " 'consider',\n",
              " 'considerable',\n",
              " 'considerably',\n",
              " 'consideration',\n",
              " 'considered',\n",
              " 'considering',\n",
              " 'considers',\n",
              " 'consistency',\n",
              " 'consistent',\n",
              " 'consistently',\n",
              " 'consisting',\n",
              " 'consists',\n",
              " 'constant',\n",
              " 'constants',\n",
              " 'constrain',\n",
              " 'constrained',\n",
              " 'constraint',\n",
              " 'constraints',\n",
              " 'construct',\n",
              " 'constructed',\n",
              " 'constructing',\n",
              " 'construction',\n",
              " 'consumption',\n",
              " 'contact',\n",
              " 'contain',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'content',\n",
              " 'context',\n",
              " 'contexts',\n",
              " 'continuity',\n",
              " 'continuous',\n",
              " 'continuously',\n",
              " 'continuum',\n",
              " 'contrast',\n",
              " 'contribute',\n",
              " 'contribution',\n",
              " 'contributions',\n",
              " 'control',\n",
              " 'controlled',\n",
              " 'controller',\n",
              " 'controlling',\n",
              " 'controls',\n",
              " 'conventional',\n",
              " 'converge',\n",
              " 'convergence',\n",
              " 'converges',\n",
              " 'convex',\n",
              " 'convolution',\n",
              " 'convolutional',\n",
              " 'coordinate',\n",
              " 'coordinates',\n",
              " 'core',\n",
              " 'corpus',\n",
              " 'correct',\n",
              " 'correction',\n",
              " 'correctly',\n",
              " 'correctness',\n",
              " 'correlated',\n",
              " 'correlation',\n",
              " 'correlations',\n",
              " 'correspond',\n",
              " 'correspondence',\n",
              " 'corresponding',\n",
              " 'corresponds',\n",
              " 'cosmic',\n",
              " 'cosmological',\n",
              " 'cost',\n",
              " 'costs',\n",
              " 'counterpart',\n",
              " 'counterparts',\n",
              " 'counting',\n",
              " 'coupled',\n",
              " 'coupling',\n",
              " 'covariance',\n",
              " 'covariates',\n",
              " 'cover',\n",
              " 'coverage',\n",
              " 'covering',\n",
              " 'create',\n",
              " 'created',\n",
              " 'creating',\n",
              " 'criteria',\n",
              " 'criterion',\n",
              " 'critical',\n",
              " 'cross',\n",
              " 'crucial',\n",
              " 'crystal',\n",
              " 'cubic',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'curvature',\n",
              " 'curve',\n",
              " 'curves',\n",
              " 'cycle',\n",
              " 'cycles',\n",
              " 'dark',\n",
              " 'data',\n",
              " 'data-driven',\n",
              " 'database',\n",
              " 'dataset',\n",
              " 'datasets',\n",
              " 'date',\n",
              " 'deal',\n",
              " 'dealing',\n",
              " 'decade',\n",
              " 'decades',\n",
              " 'decay',\n",
              " 'decision',\n",
              " 'decisions',\n",
              " 'decoding',\n",
              " 'decomposition',\n",
              " 'decrease',\n",
              " 'decreases',\n",
              " 'decreasing',\n",
              " 'deep',\n",
              " 'deeper',\n",
              " 'defects',\n",
              " 'define',\n",
              " 'defined',\n",
              " 'defines',\n",
              " 'defining',\n",
              " 'definition',\n",
              " 'deformation',\n",
              " 'degree',\n",
              " 'degrees',\n",
              " 'delay',\n",
              " 'demand',\n",
              " 'demonstrate',\n",
              " 'demonstrated',\n",
              " 'demonstrates',\n",
              " 'demonstrating',\n",
              " 'dense',\n",
              " 'densities',\n",
              " 'density',\n",
              " 'depend',\n",
              " 'dependence',\n",
              " 'dependencies',\n",
              " 'dependency',\n",
              " 'dependent',\n",
              " 'depending',\n",
              " 'depends',\n",
              " 'deployed',\n",
              " 'deployment',\n",
              " 'depth',\n",
              " 'derivative',\n",
              " 'derivatives',\n",
              " 'derive',\n",
              " 'derived',\n",
              " 'deriving',\n",
              " 'descent',\n",
              " 'described',\n",
              " 'describes',\n",
              " 'describing',\n",
              " 'description',\n",
              " 'design',\n",
              " 'designed',\n",
              " 'designing',\n",
              " 'designs',\n",
              " 'desirable',\n",
              " 'desired',\n",
              " 'despite',\n",
              " 'detailed',\n",
              " 'details',\n",
              " 'detect',\n",
              " 'detected',\n",
              " 'detecting',\n",
              " 'detection',\n",
              " 'detector',\n",
              " 'detectors',\n",
              " 'determination',\n",
              " 'determine',\n",
              " 'determined',\n",
              " 'determines',\n",
              " 'determining',\n",
              " 'deterministic',\n",
              " 'develop',\n",
              " 'developed',\n",
              " 'developing',\n",
              " 'development',\n",
              " 'developments',\n",
              " 'develops',\n",
              " 'deviation',\n",
              " 'device',\n",
              " 'devices',\n",
              " 'devise',\n",
              " 'diagnosis',\n",
              " 'diagram',\n",
              " 'dictionary',\n",
              " 'differ',\n",
              " 'difference',\n",
              " 'differences',\n",
              " 'different',\n",
              " 'differentiable',\n",
              " 'differential',\n",
              " 'difficult',\n",
              " 'difficulties',\n",
              " 'difficulty',\n",
              " 'diffusion',\n",
              " 'digital',\n",
              " 'dimension',\n",
              " 'dimensional',\n",
              " 'dimensionality',\n",
              " 'dimensions',\n",
              " 'direct',\n",
              " 'directed',\n",
              " 'direction',\n",
              " 'directions',\n",
              " 'directly',\n",
              " 'dirichlet',\n",
              " 'discover',\n",
              " 'discovered',\n",
              " 'discovery',\n",
              " 'discrete',\n",
              " 'discretization',\n",
              " 'discriminative',\n",
              " 'discuss',\n",
              " 'discussed',\n",
              " 'discusses',\n",
              " 'discussion',\n",
              " 'disease',\n",
              " 'disk',\n",
              " 'disorder',\n",
              " 'dispersion',\n",
              " 'distance',\n",
              " 'distances',\n",
              " 'distinct',\n",
              " 'distinguish',\n",
              " 'distributed',\n",
              " 'distribution',\n",
              " 'distributions',\n",
              " 'divergence',\n",
              " 'diverse',\n",
              " 'diversity',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'domain',\n",
              " 'domains',\n",
              " 'dominant',\n",
              " 'dominated',\n",
              " 'double',\n",
              " 'draw',\n",
              " 'drawn',\n",
              " 'drift',\n",
              " 'drive',\n",
              " 'driven',\n",
              " 'driving',\n",
              " 'dual',\n",
              " 'duality',\n",
              " 'dynamic',\n",
              " 'dynamical',\n",
              " 'dynamically',\n",
              " 'dynamics',\n",
              " 'e.g.',\n",
              " 'earlier',\n",
              " 'early',\n",
              " 'earth',\n",
              " 'easier',\n",
              " 'easily',\n",
              " 'easy',\n",
              " 'economic',\n",
              " 'edge',\n",
              " 'edges',\n",
              " 'effect',\n",
              " 'effective',\n",
              " 'effectively',\n",
              " 'effectiveness',\n",
              " 'effects',\n",
              " 'efficacy',\n",
              " 'efficiency',\n",
              " 'efficient',\n",
              " 'efficiently',\n",
              " 'effort',\n",
              " 'efforts',\n",
              " 'eigenvalue',\n",
              " 'eigenvalues',\n",
              " 'elastic',\n",
              " 'electric',\n",
              " 'electrical',\n",
              " 'electromagnetic',\n",
              " 'electron',\n",
              " 'electronic',\n",
              " 'electrons',\n",
              " 'element',\n",
              " 'elementary',\n",
              " 'elements',\n",
              " 'elliptic',\n",
              " 'embedded',\n",
              " 'embedding',\n",
              " 'embeddings',\n",
              " 'emerged',\n",
              " 'emergence',\n",
              " 'emerging',\n",
              " 'emission',\n",
              " 'empirical',\n",
              " 'empirically',\n",
              " 'employ',\n",
              " 'employed',\n",
              " 'employing',\n",
              " 'employs',\n",
              " 'enable',\n",
              " 'enables',\n",
              " 'enabling',\n",
              " 'encode',\n",
              " 'encoded',\n",
              " 'encoding',\n",
              " 'end-to-end',\n",
              " 'energies',\n",
              " 'energy',\n",
              " 'engineering',\n",
              " 'enhance',\n",
              " 'enhanced',\n",
              " 'enhancement',\n",
              " 'ensemble',\n",
              " 'ensure',\n",
              " 'ensuring',\n",
              " 'entire',\n",
              " 'entities',\n",
              " 'entries',\n",
              " 'entropy',\n",
              " 'environment',\n",
              " 'environmental',\n",
              " 'environments',\n",
              " 'equal',\n",
              " 'equation',\n",
              " 'equations',\n",
              " 'equilibrium',\n",
              " 'equipped',\n",
              " 'equivalence',\n",
              " 'equivalent',\n",
              " 'error',\n",
              " 'errors',\n",
              " 'especially',\n",
              " 'essential',\n",
              " 'essentially',\n",
              " 'establish',\n",
              " 'established',\n",
              " 'establishing',\n",
              " 'estimate',\n",
              " 'estimated',\n",
              " 'estimates',\n",
              " 'estimating',\n",
              " 'estimation',\n",
              " 'estimator',\n",
              " 'estimators',\n",
              " 'euclidean',\n",
              " 'euler',\n",
              " 'evaluate',\n",
              " 'evaluated',\n",
              " 'evaluating',\n",
              " 'evaluation',\n",
              " 'evaluations',\n",
              " 'event',\n",
              " 'events',\n",
              " 'evidence',\n",
              " 'evolution',\n",
              " 'evolutionary',\n",
              " 'evolving',\n",
              " 'exact',\n",
              " 'exactly',\n",
              " 'examine',\n",
              " 'examined',\n",
              " 'example',\n",
              " 'examples',\n",
              " 'excellent',\n",
              " 'exchange',\n",
              " 'excitation',\n",
              " 'excitations',\n",
              " 'execution',\n",
              " 'exhibit',\n",
              " 'exhibits',\n",
              " 'exist',\n",
              " 'existence',\n",
              " 'existing',\n",
              " 'exists',\n",
              " 'expansion',\n",
              " 'expect',\n",
              " 'expectation',\n",
              " 'expected',\n",
              " 'expensive',\n",
              " 'experience',\n",
              " 'experiment',\n",
              " 'experimental',\n",
              " 'experimentally',\n",
              " 'experiments',\n",
              " 'expert',\n",
              " 'experts',\n",
              " 'explain',\n",
              " 'explained',\n",
              " 'explains',\n",
              " 'explanation',\n",
              " 'explicit',\n",
              " 'explicitly',\n",
              " 'exploit',\n",
              " 'exploited',\n",
              " 'exploiting',\n",
              " 'exploits',\n",
              " 'exploration',\n",
              " 'explore',\n",
              " 'explored',\n",
              " 'explores',\n",
              " 'exploring',\n",
              " 'exponent',\n",
              " 'exponential',\n",
              " 'exponentially',\n",
              " 'expressed',\n",
              " 'expression',\n",
              " 'expressions',\n",
              " 'extend',\n",
              " 'extended',\n",
              " 'extending',\n",
              " 'extends',\n",
              " 'extension',\n",
              " 'extensions',\n",
              " 'extensive',\n",
              " 'extensively',\n",
              " 'extent',\n",
              " 'external',\n",
              " 'extra',\n",
              " 'extract',\n",
              " 'extracted',\n",
              " 'extraction',\n",
              " 'extreme',\n",
              " 'extremely',\n",
              " 'face',\n",
              " 'facilitate',\n",
              " 'fact',\n",
              " 'factor',\n",
              " 'factorization',\n",
              " 'factors',\n",
              " 'fail',\n",
              " 'fails',\n",
              " 'failure',\n",
              " 'false',\n",
              " 'families',\n",
              " 'family',\n",
              " 'fashion',\n",
              " 'fast',\n",
              " 'faster',\n",
              " 'feasibility',\n",
              " 'feasible',\n",
              " 'feature',\n",
              " 'features',\n",
              " 'feedback',\n",
              " 'fermi',\n",
              " 'fewer',\n",
              " 'field',\n",
              " 'fields',\n",
              " 'films',\n",
              " 'filter',\n",
              " 'filtering',\n",
              " 'filters',\n",
              " 'final',\n",
              " 'finally',\n",
              " 'financial',\n",
              " 'finding',\n",
              " 'findings',\n",
              " 'fine',\n",
              " 'finite',\n",
              " 'finitely',\n",
              " 'first-order',\n",
              " 'firstly',\n",
              " 'fitting',\n",
              " 'fixed',\n",
              " 'flat',\n",
              " 'flexibility',\n",
              " 'flexible',\n",
              " 'flow',\n",
              " 'flows',\n",
              " 'fluctuations',\n",
              " 'fluid',\n",
              " 'flux',\n",
              " 'focus',\n",
              " 'focused',\n",
              " 'focuses',\n",
              " 'focusing',\n",
              " 'follow',\n",
              " 'followed',\n",
              " 'following',\n",
              " 'follows',\n",
              " 'force',\n",
              " 'forces',\n",
              " 'forecasting',\n",
              " 'form',\n",
              " 'formal',\n",
              " 'formalism',\n",
              " 'formation',\n",
              " 'formed',\n",
              " 'forming',\n",
              " 'forms',\n",
              " 'formula',\n",
              " 'formulas',\n",
              " 'formulate',\n",
              " 'formulated',\n",
              " 'formulation',\n",
              " 'forward',\n",
              " 'foundation',\n",
              " 'fourier',\n",
              " 'fraction',\n",
              " 'fractional',\n",
              " 'frame',\n",
              " 'framework',\n",
              " 'frameworks',\n",
              " 'free',\n",
              " 'freedom',\n",
              " 'frequencies',\n",
              " 'frequency',\n",
              " 'frequently',\n",
              " 'fully',\n",
              " 'function',\n",
              " 'functional',\n",
              " 'functionals',\n",
              " 'functions',\n",
              " 'fundamental',\n",
              " 'furthermore',\n",
              " 'fusion',\n",
              " 'future',\n",
              " 'gain',\n",
              " 'gains',\n",
              " 'galactic',\n",
              " 'galaxies',\n",
              " 'galaxy',\n",
              " 'game',\n",
              " 'games',\n",
              " 'gans',\n",
              " 'gaps',\n",
              " 'gaussian',\n",
              " 'gene',\n",
              " 'general',\n",
              " 'generalization',\n",
              " 'generalizations',\n",
              " 'generalize',\n",
              " 'generalized',\n",
              " 'generalizes',\n",
              " 'generally',\n",
              " 'generate',\n",
              " 'generated',\n",
              " 'generates',\n",
              " 'generating',\n",
              " 'generation',\n",
              " 'generative',\n",
              " 'generator',\n",
              " 'generic',\n",
              " 'genetic',\n",
              " 'geometric',\n",
              " 'geometry',\n",
              " 'given',\n",
              " 'gives',\n",
              " 'giving',\n",
              " 'global',\n",
              " 'globally',\n",
              " 'goal',\n",
              " 'goals',\n",
              " 'goes',\n",
              " 'good',\n",
              " 'gradient',\n",
              " 'gradients',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DNr9Cc7oVMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = decomposition.LatentDirichletAllocation(n_components=6,max_iter=3,learning_method='online',learning_offset=50,n_jobs=1,random_state=111)\n",
        "\n",
        "W1 = lda.fit_transform(tf_vectors)\n",
        "H1 = lda.components_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FycGZFW5ptGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "251e3fa3-8b96-4042-b4da-54bbc54f53bf"
      },
      "source": [
        "W1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4828631 , 0.10266479, 0.29767638, 0.113707  , 0.00154643,\n",
              "        0.0015423 ],\n",
              "       [0.00506452, 0.9746415 , 0.00506283, 0.00506728, 0.00507799,\n",
              "        0.00508588],\n",
              "       [0.94230384, 0.00476887, 0.00479106, 0.00478408, 0.004815  ,\n",
              "        0.03853714],\n",
              "       ...,\n",
              "       [0.00364935, 0.83559348, 0.00365458, 0.00367364, 0.00366272,\n",
              "        0.14976624],\n",
              "       [0.00700834, 0.00699229, 0.00700247, 0.00698802, 0.6965687 ,\n",
              "        0.27544018],\n",
              "       [0.00429637, 0.87271948, 0.11008643, 0.00429068, 0.00430344,\n",
              "        0.0043036 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6HvVICXpuon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 20\n",
        "\n",
        "vocab = np.array(vectorizer_tf.get_feature_names())\n",
        "\n",
        "top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
        "topic_words = ([top_words(t) for t in H1])\n",
        "topics = [' '.join(t) for t in topic_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97rJJLegqnVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "691312d7-379b-4905-e191-099fa30b3f32"
      },
      "source": [
        "topics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data model paper different analysis information using network results systems time used networks based research social study approach users work',\n",
              " 'prove problem paper study results graph \\\\mathbb result space functions \\\\mathcal function number graphs group given bound case finite theorem',\n",
              " 'learning network neural networks data model deep training method models methods performance using propose approach machine proposed classification paper based',\n",
              " 'algorithm data problem model method distribution proposed models based optimal methods paper algorithms approach estimation random time optimization results number',\n",
              " 'systems quantum energy theory states equations equation numerical model flow method dynamics solutions results nonlinear order using time scheme boundary',\n",
              " 'phase model field magnetic using temperature density results mass high observed surface energy state properties range different effect study effects']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZMojnQKqozx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(X_train.ABSTRACT))]\n",
        "\n",
        "df_doc_topic = pd.DataFrame(np.round(W1,2),columns=colnames,index=docnames)\n",
        "significanttopic = np.argmax(df_doc_topic.values,axis=1)\n",
        "\n",
        "df_doc_topic['dominant_topic'] = significanttopic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upHoOP3Tr-mU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f880035b-b1ab-4fe7-dfa9-4d7dba950e8b"
      },
      "source": [
        "df_doc_topic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0</th>\n",
              "      <th>Topic1</th>\n",
              "      <th>Topic2</th>\n",
              "      <th>Topic3</th>\n",
              "      <th>Topic4</th>\n",
              "      <th>Topic5</th>\n",
              "      <th>dominant_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10501</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10502</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10503</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10504</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.28</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc10505</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10506 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Topic0  Topic1  Topic2  Topic3  Topic4  Topic5  dominant_topic\n",
              "Doc0      0.48    0.10    0.30    0.11    0.00    0.00    0             \n",
              "Doc1      0.01    0.97    0.01    0.01    0.01    0.01    1             \n",
              "Doc2      0.94    0.00    0.00    0.00    0.00    0.04    0             \n",
              "Doc3      0.00    0.00    0.00    0.98    0.00    0.00    3             \n",
              "Doc4      0.00    0.31    0.00    0.55    0.00    0.14    3             \n",
              "...        ...     ...     ...     ...     ...     ...   ..             \n",
              "Doc10501  0.50    0.00    0.25    0.11    0.00    0.14    0             \n",
              "Doc10502  0.01    0.89    0.09    0.01    0.01    0.01    1             \n",
              "Doc10503  0.00    0.84    0.00    0.00    0.00    0.15    1             \n",
              "Doc10504  0.01    0.01    0.01    0.01    0.70    0.28    4             \n",
              "Doc10505  0.00    0.87    0.11    0.00    0.00    0.00    1             \n",
              "\n",
              "[10506 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3-5QqaUsArk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7746478-3ea0-4e54-b048-5249b18fe85c"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12536</th>\n",
              "      <td>9998</td>\n",
              "      <td>Network-based methods for outcome prediction in the \"sample space\"</td>\n",
              "      <td>In this thesis we present the novel semi-supervised network-based algorithm\\nP-Net, which is able to rank and classify patients with respect to a specific\\nphenotype or clinical outcome under study. The peculiar and innovative\\ncharacteristic of this method is that it builds a network of samples/patients,\\nwhere the nodes represent the samples and the edges are functional or genetic\\nrelationships between individuals (e.g. similarity of expression profiles), to\\npredict the phenotype under study. In other words, it constructs the network in\\nthe \"sample space\" and not in the \"biomarker space\" (where nodes represent\\nbiomolecules (e.g. genes, proteins) and edges represent functional or genetic\\nrelationships between nodes), as usual in state-of-the-art methods. To assess\\nthe performances of P-Net, we apply it on three different publicly available\\ndatasets from patients afflicted with a specific type of tumor: pancreatic\\ncancer, melanoma and ovarian cancer dataset, by using the data and following\\nthe experimental set-up proposed in two recently published papers [Barter et\\nal., 2014, Winter et al., 2012]. We show that network-based methods in the\\n\"sample space\" can achieve results competitive with classical supervised\\ninductive systems. Moreover, the graph representation of the samples can be\\neasily visualized through networks and can be used to gain visual clues about\\nthe relationships between samples, taking into account the phenotype associated\\nor predicted for each sample. To our knowledge this is one of the first works\\nthat proposes graph-based algorithms working in the \"sample space\" of the\\nbiomolecular profiles of the patients to predict their phenotype or outcome,\\nthus contributing to a novel research line in the framework of the Network\\nMedicine.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22687</th>\n",
              "      <td>18103</td>\n",
              "      <td>Existence and nonexistence of positive solutions to some fully nonlinear equation in one dimension</td>\n",
              "      <td>In this paper, we consider the existence (and nonexistence) of solutions to\\n\\[\\n-\\mathcal{M}_{\\lambda,\\Lambda}^\\pm (u'') + V(x) u = f(u) \\quad {\\rm in} \\\\n\\mathbf{R}\\n\\] where $\\mathcal{M}_{\\lambda,\\Lambda}^+$ and\\n$\\mathcal{M}_{\\lambda,\\Lambda}^-$ denote the Pucci operators with $0&lt; \\lambda\\n\\leq \\Lambda &lt; \\infty$, $V(x)$ is a bounded function, $f(s)$ is a continuous\\nfunction and its typical example is a power-type nonlinearity $f(s)\\n=|s|^{p-1}s$ $(p&gt;1)$. In particular, we are interested in positive solutions\\nwhich decay at infinity, and the existence (and nonexistence) of such solutions\\nis proved.\\n</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24588</th>\n",
              "      <td>19647</td>\n",
              "      <td>Doing Things Twice (Or Differently): Strategies to Identify Studies for Targeted Validation</td>\n",
              "      <td>The \"reproducibility crisis\" has been a highly visible source of scientific\\ncontroversy and dispute. Here, I propose and review several avenues for\\nidentifying and prioritizing research studies for the purpose of targeted\\nvalidation. Of the various proposals discussed, I identify scientific data\\nscience as being a strategy that merits greater attention among those\\ninterested in reproducibility. I argue that the tremendous potential of\\nscientific data science for uncovering high-value research studies is a\\nsignificant and rarely discussed benefit of the transition to a fully\\nopen-access publishing model.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22215</th>\n",
              "      <td>17727</td>\n",
              "      <td>Shrinkage Estimation Strategies in Generalized Ridge Regression Models Under Low/High-Dimension Regime</td>\n",
              "      <td>In this study, we propose shrinkage methods based on {\\it generalized ridge\\nregression} (GRR) estimation which is suitable for both multicollinearity and\\nhigh dimensional problems with small number of samples (large $p$, small $n$).\\nAlso, it is obtained theoretical properties of the proposed estimators for\\nLow/High Dimensional cases. Furthermore, the performance of the listed\\nestimators is demonstrated by both simulation studies and real-data analysis,\\nand compare its performance with existing penalty methods. We show that the\\nproposed methods compare well to competing regularization techniques.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21840</th>\n",
              "      <td>17434</td>\n",
              "      <td>Concurrency and Probability: Removing Confusion, Compositionally</td>\n",
              "      <td>Assigning a satisfactory truly concurrent semantics to Petri nets with\\nconfusion and distributed decisions is a long standing problem, especially if\\none wants to fully replace nondeterminism with probability distributions and no\\nstochastic structure is desired/allowed. Here we propose a general solution\\nbased on a recursive, static decomposition of (finite, occurrence) nets in loci\\nof decision, called structural branching cells (s-cells). Each s-cell exposes a\\nset of alternatives, called transactions, that can be equipped with a general\\nprobabilistic distribution. The solution is formalised as a transformation from\\na given Petri net to another net whose transitions are the transactions of the\\ns-cells and whose places are the places of the original net, with some\\nauxiliary structure for bookkeeping. The resulting net is confusion-free,\\nnamely if a transition is enabled, then all its conflicting alternatives are\\nalso enabled. Thus sets of conflicting alternatives can be equipped with\\nprobability distributions, while nonintersecting alternatives are purely\\nconcurrent and do not introduce any nondeterminism: they are Church-Rosser and\\ntheir probability distributions are independent. The validity of the\\nconstruction is witnessed by a tight correspondence result with the recent\\napproach by Abbes and Benveniste (AB) based on recursively stopped\\nconfigurations in event structures. Some advantages of our approach over AB's\\nare that: i) s-cells are defined statically and locally in a compositional way,\\nwhereas AB's branching cells are defined dynamically and globally; ii) their\\nrecursively stopped configurations correspond to possible executions, but the\\nexisting concurrency is not made explicit. Instead, our resulting nets are\\nequipped with an original concurrency structure exhibiting a so-called complete\\nconcurrency property.\\n</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...          CATEGORY\n",
              "12536  9998   ...  Statistics      \n",
              "22687  18103  ...  Mathematics     \n",
              "24588  19647  ...  Computer Science\n",
              "22215  17727  ...  Statistics      \n",
              "21840  17434  ...  Computer Science\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMRsS5ZDsH4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WHold = lda.transform(vectorizer_tf.transform(X_hold.ABSTRACT[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N88PLCeusX-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(X_hold.ABSTRACT[:5]))]\n",
        "\n",
        "df_doc_topics = pd.DataFrame(np.round(WHold,2),columns=colnames,index=docnames)\n",
        "significanttopic = np.argmax(df_doc_topics.values,axis=1)\n",
        "\n",
        "df_doc_topics['dominant_topic'] = significanttopic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rark9j-8sg-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3b11f438-319b-405f-8fcd-ba4fa215b62f"
      },
      "source": [
        "df_doc_topics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0</th>\n",
              "      <th>Topic1</th>\n",
              "      <th>Topic2</th>\n",
              "      <th>Topic3</th>\n",
              "      <th>Topic4</th>\n",
              "      <th>Topic5</th>\n",
              "      <th>dominant_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Topic0  Topic1  Topic2  Topic3  Topic4  Topic5  dominant_topic\n",
              "Doc0  0.36    0.00    0.10    0.54    0.0     0.00    3             \n",
              "Doc1  0.00    0.00    0.12    0.87    0.0     0.00    3             \n",
              "Doc2  0.23    0.05    0.37    0.32    0.0     0.03    2             \n",
              "Doc3  0.00    0.00    0.99    0.00    0.0     0.00    2             \n",
              "Doc4  0.75    0.15    0.00    0.10    0.0     0.00    0             "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxfvfY5NsiTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e32227b-015e-49b2-df2c-0a9e80d949f3"
      },
      "source": [
        "X_hold.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9439</th>\n",
              "      <td>7492</td>\n",
              "      <td>In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential Importance Sampling</td>\n",
              "      <td>Entity resolution (ER) presents unique challenges for evaluation methodology.\\nWhile crowdsourcing platforms acquire ground truth, sound approaches to\\nsampling must drive labelling efforts. In ER, extreme class imbalance between\\nmatching and non-matching records can lead to enormous labelling requirements\\nwhen seeking statistically consistent estimates for rigorous evaluation. This\\npaper addresses this important challenge with the OASIS algorithm: a sampler\\nand F-measure estimator for ER evaluation. OASIS draws samples from a (biased)\\ninstrumental distribution, chosen to ensure estimators with optimal asymptotic\\nvariance. As new labels are collected OASIS updates this instrumental\\ndistribution via a Bayesian latent variable model of the annotator oracle, to\\nquickly focus on unlabelled items providing more information. We prove that\\nresulting estimates of F-measure, precision, recall converge to the true\\npopulation values. Thorough comparisons of sampling methods on a variety of ER\\ndatasets demonstrate significant labelling reductions of up to 83% without loss\\nto estimate accuracy.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10785</th>\n",
              "      <td>8583</td>\n",
              "      <td>Subspace Clustering with Missing and Corrupted Data</td>\n",
              "      <td>Given full or partial information about a collection of points that lie close\\nto a union of several subspaces, subspace clustering refers to the process of\\nclustering the points according to their subspace and identifying the\\nsubspaces. One popular approach, sparse subspace clustering (SSC), represents\\neach sample as a weighted combination of the other samples, with weights of\\nminimal $\\ell_1$ norm, and then uses those learned weights to cluster the\\nsamples. SSC is stable in settings where each sample is contaminated by a\\nrelatively small amount of noise. However, when there is a significant amount\\nof additive noise, or a considerable number of entries are missing, theoretical\\nguarantees are scarce. In this paper, we study a robust variant of SSC and\\nestablish clustering guarantees in the presence of corrupted or missing data.\\nWe give explicit bounds on amount of noise and missing data that the algorithm\\ncan tolerate, both in deterministic settings and in a random generative model.\\nNotably, our approach provides guarantees for higher tolerance to noise and\\nmissing data than existing analyses for this method. By design, the results\\nhold even when we do not know the locations of the missing data; e.g., as in\\npresence-only data.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12291</th>\n",
              "      <td>9804</td>\n",
              "      <td>A Brief Introduction to Machine Learning for Engineers</td>\n",
              "      <td>This monograph aims at providing an introduction to key concepts, algorithms,\\nand theoretical results in machine learning. The treatment concentrates on\\nprobabilistic models for supervised and unsupervised learning problems. It\\nintroduces fundamental concepts and algorithms by building on first principles,\\nwhile also exposing the reader to more advanced topics with extensive pointers\\nto the literature, within a unified notation and mathematical framework. The\\nmaterial is organized according to clearly defined categories, such as\\ndiscriminative and generative models, frequentist and Bayesian approaches,\\nexact and approximate inference, as well as directed and undirected models.\\nThis monograph is meant as an entry point for researchers with a background in\\nprobability and linear algebra.\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13040</th>\n",
              "      <td>10396</td>\n",
              "      <td>Building Robust Deep Neural Networks for Road Sign Detection</td>\n",
              "      <td>Deep Neural Networks are built to generalize outside of training set in mind\\nby using techniques such as regularization, early stopping and dropout. But\\nconsiderations to make them more resilient to adversarial examples are rarely\\ntaken. As deep neural networks become more prevalent in mission-critical and\\nreal-time systems, miscreants start to attack them by intentionally making deep\\nneural networks to misclassify an object of one type to be seen as another\\ntype. This can be catastrophic in some scenarios where the classification of a\\ndeep neural network can lead to a fatal decision by a machine. In this work, we\\nused GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method\\nand Jacobian Saliency Method, used those crafted adversarial samples to attack\\nanother Deep Convolutional Neural Network and built the attacked network to be\\nmore resilient against adversarial attacks by making it more robust by\\nDefensive Distillation and Adversarial Training\\n</td>\n",
              "      <td>Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16397</th>\n",
              "      <td>13081</td>\n",
              "      <td>Influence Networks in International Relations</td>\n",
              "      <td>Measuring influence and determining what drives it are persistent questions\\nin political science and in network analysis more generally. Herein we focus on\\nthe domain of international relations. Our major substantive question is: How\\ncan we determine what characteristics make an actor influential? To address the\\ntopic of influence, we build on a multilinear tensor regression framework\\n(MLTR) that captures influence relationships using a tensor generalization of a\\nvector autoregression model. Influence relationships in that approach are\\ncaptured in a pair of n x n matrices and provide measurements of how the\\nnetwork actions of one actor may influence the future actions of another. A\\nlimitation of the MLTR and earlier latent space approaches is that there are no\\ndirect mechanisms through which to explain why a certain actor is more or less\\ninfluential than others. Our new framework, social influence regression,\\nprovides a way to statistically model the influence of one actor on another as\\na function of characteristics of the actors. Thus we can move beyond just\\nestimating that an actor influences another to understanding why. To highlight\\nthe utility of this approach, we apply it to studying monthly-level conflictual\\nevents between countries as measured through the Integrated Crisis Early\\nWarning System (ICEWS) event data project.\\n</td>\n",
              "      <td>Physics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...    CATEGORY\n",
              "9439   7492   ...  Statistics\n",
              "10785  8583   ...  Statistics\n",
              "12291  9804   ...  Statistics\n",
              "13040  10396  ...  Statistics\n",
              "16397  13081  ...  Physics   \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq6oM3GXt1YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}